import torch  
import torch.nn as nn  
from torchvision import transforms,datasets  
from torch.utils.data import  DataLoader,Dataset,random_split  
  
transform = transforms.Compose([  
    transforms.ToTensor(),  
    transforms.Normalize((0.5,),(0.5,))  
])  
  
training_data  = datasets.MNIST(root="./data",train=True,download=True,transform=transform)  
testing_data  = datasets.MNIST(root="./data",train=False,download=True,transform=transform)  
  
splitter = int(0.8*len(training_data))  
validation_data,training_data = random_split(training_data,[splitter,len(training_data)-splitter])  
  
training_loader = DataLoader(training_data,shuffle=True,batch_size=300)  
validation_loader = DataLoader(validation_data,shuffle=False,batch_size=300)  
testing_loader = DataLoader(testing_data,shuffle=False,batch_size=300)  
  
'''image,label = next(iter(training_loader))  
  
lack = image.shape  
print(lack)'''  
  
# Starting at shape [300,1,28,28]  
  
class ConvolutionalNet(nn.Module):  
    def __init__(self):  
        super().__init__()  
        self.layers = nn.Sequential(  
            nn.Conv2d(1,10,kernel_size=3,padding=1),  
            nn.ReLU(),  
            nn.MaxPool2d(2),  
  
            nn.Conv2d(10,100,kernel_size=3,padding=1),  
            nn.ReLU(),  
            nn.MaxPool2d(2),  
  
            nn.Conv2d(100,200,kernel_size=3,padding=1),  
            nn.ReLU(),  
            nn.MaxPool2d(2),  
  
            nn.Flatten(),  
  
            nn.Linear(1800,20),  
            nn.ReLU(),  
            nn.Dropout(0.5),  
            nn.Linear(20,10)  
        )  
  
    def forward(self,x):  
        return self.layers(x)  
  
model  = ConvolutionalNet()  
  
  
'''dummy_data = torch.rand(300,1,28,28)  
  
fake_model = nn.Sequential(  
    model.layers[0],    model.layers[1],    model.layers[2],    model.layers[3],    model.layers[4],    model.layers[5],    model.layers[6],    model.layers[7],    model.layers[8],    model.layers[9])  
  
before_flatten = fake_model(dummy_data)  
print(f"Use this shape (the one on the far right) e.g [300,1800]--1800 goes to the linear layer:{before_flatten.shape}")  
'''  
  
  
optimizer = torch.optim.Adam(model.parameters(),lr=0.01)  
loss_fn = nn.CrossEntropyLoss()  
  
loss = 0  
model.train()     
for epoch in range(100):  
    for images,labels in training_loader:  
        y_predicted = model(images)  
        loss = loss_fn(y_predicted,labels)  
        loss += loss  
        optimizer.zero_grad()  
        loss.backward()  
        optimizer.step()  
    print(f"Epoch:{epoch+1}__Loss:{loss}")  
  
  
model.eval()  
with torch.inference_mode():  
    for x_batch,y_batch in testing_loader:  
        prediction = model(x_batch)  
        loss = loss_fn(prediction,y_batch)  
        print(f"Testing Loss:\n")  
        print(loss)  
          
          
model.eval()  
with torch.inference_mode():  
    for x_batch,y_batch in validation_loader:  
        prediction = model(x_batch)  
        loss = loss_fn(prediction,y_batch)  
        print(f"Validation Loss:\n")  
        print(loss)
